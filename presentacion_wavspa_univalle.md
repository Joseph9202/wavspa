# WavSpA: Wavelet Space Attention
## Transformers Eficientes mediante An√°lisis Multi-Escala

**Seminario del Departamento de Matem√°ticas**  
Universidad del Valle  
Octubre 2025

---

## üìã Agenda

1. **Motivaci√≥n**: El problema de las secuencias largas
2. **Fundamentos Matem√°ticos**: Teor√≠a de Wavelets
3. **Arquitectura WavSpA**: Dise√±o e Implementaci√≥n
4. **An√°lisis Te√≥rico**: Complejidad y Propiedades
5. **Resultados Experimentales**: Long Range Arena
6. **C√≥digo**: An√°lisis de Implementaci√≥n
7. **Conclusiones y Trabajo Futuro**

---

# 1. Motivaci√≥n

---

## El Problema: Complejidad Cuadr√°tica en Transformers

### Transformer Est√°ndar (Vaswani et al., 2017)

**Mecanismo de Atenci√≥n:**

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

donde:
- $Q, K, V \in \mathbb{R}^{L \times d}$ (Queries, Keys, Values)
- $L$ = longitud de secuencia
- $d$ = dimensi√≥n de embedding

---

## An√°lisis de Complejidad

### Producto Matricial $QK^T$:

$$
QK^T \in \mathbb{R}^{L \times L}
$$

**Complejidad Computacional:**
- Tiempo: $O(L^2 \cdot d)$
- Memoria: $O(L^2)$

**Problema:** 
- $L = 512$: ~260K operaciones
- $L = 4096$: ~16M operaciones (61√ó m√°s!)
- $L = 16384$: ~268M operaciones (1000√ó m√°s!)

‚ùå **No escalable** para secuencias largas

---

## Aplicaciones Requieren Secuencias Largas

| Tarea | Longitud T√≠pica | Desaf√≠o |
|-------|----------------|---------|
| Resumen de documentos | 4K-16K tokens | Capturar narrativa completa |
| An√°lisis de c√≥digo | 8K-32K tokens | Dependencias de largo alcance |
| Gen√≥mica | 100K-1M bases | Patrones regulatorios distantes |
| Series temporales | 10K-100K puntos | Tendencias de largo plazo |
| Audio/Video | 48K-192K frames | Coherencia temporal |

**Necesidad:** Modelos eficientes para $L \gg 4096$

---

# 2. Fundamentos Matem√°ticos

---

## Teor√≠a de Wavelets: Intuici√≥n

### ¬øQu√© es una Wavelet?

Una **wavelet** $\psi(t)$ es una "ondita" localizada:

$$
\int_{-\infty}^{\infty} \psi(t) \, dt = 0 \quad \text{(media cero)}
$$

$$
\int_{-\infty}^{\infty} |\psi(t)|^2 \, dt < \infty \quad \text{(energ√≠a finita)}
$$

**Familia de Wavelets:**

$$
\psi_{a,b}(t) = \frac{1}{\sqrt{|a|}} \psi\left(\frac{t-b}{a}\right)
$$

- $a$: escala (frecuencia)
- $b$: translaci√≥n (posici√≥n)

---

## Transformada Wavelet: An√°lisis Multi-Escala

### Transformada Wavelet Continua (CWT):

$$
W_f(a, b) = \int_{-\infty}^{\infty} f(t) \, \frac{1}{\sqrt{|a|}} \psi^*\left(\frac{t-b}{a}\right) dt
$$

**Interpretaci√≥n:**
- Mide similitud entre se√±al $f(t)$ y wavelet en escala $a$, posici√≥n $b$
- Plano tiempo-frecuencia: $(b, a) \mapsto W_f(a,b)$

### Transformada Wavelet Discreta (DWT):

$$
c_j[k] = \sum_n f[n] \, h_j[2^j k - n]
$$

**Banco de Filtros:**
- $h_0$: filtro paso-bajo (aproximaci√≥n)
- $h_1$: filtro paso-alto (detalles)

---

## Descomposici√≥n Multi-Resoluci√≥n

### Esquema de Descomposici√≥n (3 niveles):

```
Se√±al Original [L puntos]
        |
        ‚îú‚îÄ [h0] ‚Üí Aproximaci√≥n A‚ÇÅ [L/2] ‚îÄ‚îÄ‚îê
        |                                  |
        ‚îî‚îÄ [h1] ‚Üí Detalle D‚ÇÅ [L/2]         |
                                           |
                  A‚ÇÅ [L/2]                 |
                    |                      |
                    ‚îú‚îÄ [h0] ‚Üí A‚ÇÇ [L/4] ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                    |                      |
                    ‚îî‚îÄ [h1] ‚Üí D‚ÇÇ [L/4]     |
                                           |
                        A‚ÇÇ [L/4]           |
                          |                |
                          ‚îú‚îÄ [h0] ‚Üí A‚ÇÉ [L/8]
                          |
                          ‚îî‚îÄ [h1] ‚Üí D‚ÇÉ [L/8]

Resultado: {A‚ÇÉ, D‚ÇÉ, D‚ÇÇ, D‚ÇÅ}
```

**Propiedad Clave:** Reconstrucci√≥n perfecta
$$
f = \text{IDWT}(\text{DWT}(f))
$$

---

## Wavelets de Daubechies

### Construcci√≥n de Daubechies (orden $N$):

**Problema:** Encontrar $h_0 = [h_0, h_1, \ldots, h_{2N-1}]$ tal que:

1. **Ortogonalidad:**
   $$
   \sum_k h_0[k] h_0[k-2m] = \delta_m
   $$

2. **Momentos Nulos:**
   $$
   \sum_k k^p h_1[k] = 0, \quad p = 0, 1, \ldots, N-1
   $$

3. **Normalizaci√≥n:**
   $$
   \sum_k h_0[k] = \sqrt{2}
   $$

**Soluci√≥n:** Factorizaci√≥n de Fej√©r-Riesz del polinomio:

$$
P(\omega) = \left(\frac{1 + e^{i\omega}}{2}\right)^N Q(\omega)
$$

donde $Q(\omega)$ tiene ra√≠ces espec√≠ficas.

---

## Visualizaci√≥n: db2, db4, db8

### Daubechies orden 2 (db2):

```
Filtro: [‚àí0.129, 0.224, 0.836, 0.483]
Soporte: 4 coeficientes
```

### Daubechies orden 4 (db4):

```
Filtro: [‚àí0.010, ‚àí0.132, 0.047, 0.787, 0.607, ‚àí0.165, ‚àí0.072, 0.020]
Soporte: 8 coeficientes
Suavidad: ‚Üë‚Üë
```

### Daubechies orden 8 (db8):

```
Soporte: 16 coeficientes
Suavidad: ‚Üë‚Üë‚Üë‚Üë
```

**Trade-off:** M√°s suavidad ‚Üî M√°s soporte (menos localizaci√≥n)

---

# 3. Arquitectura WavSpA

---

## Idea Central: Atenci√≥n en Dominio Wavelet

### Pipeline Completo:

```
Input Sequence x ‚àà ‚Ñù^(B√óL√óD)
        ‚Üì
   [LayerNorm]
        ‚Üì
   [Wavelet Forward Transform]
        ‚Üì
   {z‚ÇÄ, z‚ÇÅ, z‚ÇÇ, ..., z_J}
   [L/1] [L/2] [L/4]    [L/2^J]
        ‚Üì    ‚Üì    ‚Üì         ‚Üì
   [Attn] [Attn] [Attn] ... [Attn]
        ‚Üì    ‚Üì    ‚Üì         ‚Üì
   {z'‚ÇÄ, z'‚ÇÅ, z'‚ÇÇ, ..., z'_J}
        ‚Üì
   [Wavelet Inverse Transform]
        ‚Üì
   x' ‚àà ‚Ñù^(B√óL√óD)
        ‚Üì
   [x + x'] (Residual)
        ‚Üì
   [MLP Block]
        ‚Üì
   Output
```

**Intuici√≥n:** Procesar diferentes escalas con diferentes "ventanas de contexto"

---

## Matem√°tica Formal: Descomposici√≥n

### Forward Transform (J niveles):

$$
\begin{align}
z_0^{(0)} &= x \\
z_j^{(k+1)} &= (z_j^{(k)} * h_0) \downarrow 2 \quad \text{(aproximaci√≥n)} \\
d_j^{(k+1)} &= (z_j^{(k)} * h_1) \downarrow 2 \quad \text{(detalles)}
\end{align}
$$

donde $\downarrow 2$ denota downsampling por factor 2.

**Resultado:**
$$
\{z_J, d_J, d_{J-1}, \ldots, d_1\}
$$

- $z_J \in \mathbb{R}^{B \times (L/2^J) \times D}$: frecuencias bajas (tendencias globales)
- $d_j \in \mathbb{R}^{B \times (L/2^j) \times D}$: frecuencias altas (detalles nivel $j$)

---

## Atenci√≥n Multi-Escala

### Para cada nivel $j \in \{0, 1, \ldots, J\}$:

$$
z'_j = \text{SelfAttention}(z_j, z_j, z_j) + z_j
$$

**Complejidad por nivel:**

| Nivel | Longitud | Complejidad Atenci√≥n | 
|-------|----------|----------------------|
| 0 (original) | $L$ | $O(L^2 \cdot D)$ |
| 1 | $L/2$ | $O((L/2)^2 \cdot D) = O(L^2 \cdot D)/4$ |
| 2 | $L/4$ | $O((L/4)^2 \cdot D) = O(L^2 \cdot D)/16$ |
| ... | ... | ... |
| J | $L/2^J$ | $O((L/2^J)^2 \cdot D)$ |

**Total:**
$$
\text{Cost} = O(L^2 D) \sum_{j=0}^{J} \frac{1}{4^j} = O(L^2 D) \cdot \frac{4}{3} \approx O(L^2 D)
$$

**Pero:** Si aplicamos atenci√≥n eficiente (Linformer, Performer) en niveles m√°s largos...

---

## Inverse Transform: Reconstrucci√≥n

### Algoritmo de Reconstrucci√≥n:

$$
\begin{align}
z_j^{(k)} &= (z_j^{(k+1)} \uparrow 2) * g_0 + (d_j^{(k+1)} \uparrow 2) * g_1
\end{align}
$$

donde:
- $\uparrow 2$: upsampling (insertar ceros)
- $g_0, g_1$: filtros de reconstrucci√≥n

**Condici√≥n de Reconstrucci√≥n Perfecta:**
$$
H_0(z)G_0(z) + H_1(z)G_1(z) = 2z^{-\ell}
$$

para alg√∫n retardo $\ell$.

**Wavelets Ortogonales:** $g_0[n] = (-1)^n h_1[1-n]$, $g_1[n] = (-1)^{n+1} h_0[1-n]$

---

## Tres Variantes de WavSpA

### 1. **AdaWavSpA**: Wavelets Adaptativas

Par√°metros entrenables:
$$
h_0^{(adapt)} \in \mathbb{R}^{w_{len} \times D}
$$

Inicializaci√≥n: Daubechies $db_N$

**Pros:** M√°xima flexibilidad  
**Contras:** Puede perder ortogonalidad

---

### 2. **OrthoWavSpA**: Wavelets Ortogonales Parametrizadas

Construcci√≥n mediante rotaciones Givens:

$$
h_0 = e_1 \cdot R(\theta_1) S \cdot R(\theta_2) S \cdots R(\theta_L) S \cdot S^{-1}
$$

donde:
- $R(\theta) = \begin{pmatrix} \sin\theta & \cos\theta \\ \cos\theta & -\sin\theta \end{pmatrix}$
- $S$: permutaci√≥n c√≠clica

Par√°metros: $\theta \in [0, 2\pi]^{L/2}$

**Pros:** Garantiza ortogonalidad  
**Contras:** Restricci√≥n puede limitar expresividad

---

### 3. **LiftWavSpA**: Lifting Scheme

Descomposici√≥n alternativa:
$$
\begin{align}
s^{(j+1)}[k] &= x[2k] \\
d^{(j+1)}[k] &= x[2k+1] - P(s^{(j+1)}[k]) \quad \text{(Predict)} \\
s'^{(j+1)}[k] &= s^{(j+1)}[k] + U(d^{(j+1)}[k]) \quad \text{(Update)}
\end{align}
$$

Par√°metros entrenables: $P$ y $U$ (redes convolucionales)

**Pros:** In-place, memoria eficiente  
**Contras:** M√°s complejo de implementar

---

# 4. An√°lisis Te√≥rico

---

## An√°lisis de Complejidad

### Transformer Est√°ndar:

| Operaci√≥n | Complejidad |
|-----------|-------------|
| Self-Attention | $O(L^2 \cdot D)$ |
| Memoria | $O(L^2 + L \cdot D)$ |

### WavSpA (J niveles):

| Operaci√≥n | Complejidad |
|-----------|-------------|
| Wavelet Transform | $O(L \cdot w_{len} \cdot D)$ |
| Multi-Scale Attention | $O(L^2 \cdot D \cdot \frac{4}{3})$ |
| Inverse Transform | $O(L \cdot w_{len} \cdot D)$ |
| **Total** | $\mathbf{O(L^2 \cdot D + L \cdot w_{len} \cdot D)}$ |

**Con atenci√≥n lineal (e.g., Performer):**

$$
\text{Total} = O(L \cdot D^2 \cdot \frac{4}{3} + L \cdot w_{len} \cdot D) = O(L \cdot D^2)
$$

‚úÖ **Lineal en L!**

---

## Teorema: Capacidad Representacional

**Teorema (Informal):** 

Para cualquier funci√≥n $f: \mathbb{R}^L \to \mathbb{R}^L$ expresable por un Transformer de profundidad $N$, existe un WavSpA de profundidad $O(N)$ que puede aproximar $f$ con precisi√≥n arbitraria, siempre que:

1. Las wavelets sean suficientemente regulares (e.g., $db_4$ o superior)
2. El n√∫mero de niveles $J \geq \log_2(L)$
3. Cada nivel tenga atenci√≥n de capacidad equivalente

**Intuici√≥n:**
- Wavelets descomponen en espacios anidados
- Atenci√≥n captura correlaciones
- Reconstrucci√≥n combina informaci√≥n

**Referencia:** Similar a resultados de aproximaci√≥n universal para redes wavelet (Zhang, 1993)

---

## Propiedad: Localidad e Invarianza

### 1. **Localidad Espacio-Frecuencia:**

Por principio de incertidumbre de Heisenberg:
$$
\Delta t \cdot \Delta \omega \geq \frac{1}{2}
$$

Wavelets logran un balance √≥ptimo (para Gaussianas).

**Implicaci√≥n:** 
- Detalles $d_j$: localizados espacialmente
- Aproximaci√≥n $z_J$: informaci√≥n global

### 2. **Invarianza a Desplazamientos:**

Wavelet Packet Transform (extensi√≥n de WavSpA):
$$
\text{WPT}(\tau_s f) \approx \tau_{s/2^j} \text{WPT}(f)
$$

donde $\tau_s$ es desplazamiento de $s$ posiciones.

**Ventaja:** Robustez a variaciones de posici√≥n

---

# 5. Resultados Experimentales

---

## Long Range Arena (LRA) Benchmark

### Suite de 6 tareas para evaluar secuencias largas:

| Tarea | Longitud | Descripci√≥n | M√©trica |
|-------|----------|-------------|---------|
| **ListOps** | 2K | Evaluaci√≥n de expresiones anidadas | Accuracy |
| **Text** | 4K | Clasificaci√≥n de texto (IMDB) | Accuracy |
| **Retrieval** | 4K | B√∫squeda documento-query (AAN) | Accuracy |
| **Image** | 1K | Clasificaci√≥n CIFAR-10 (p√≠xeles) | Accuracy |
| **Pathfinder** | 1K | Conectividad visual (largo alcance) | Accuracy |
| **Avg** | - | Promedio de 5 tareas | Accuracy |

**Desaf√≠o:** Capturar dependencias de 1K-4K tokens

---

## Resultados: WavSpA vs. Transformer Base

### Tabla de Resultados (%):

| Modelo | ListOps | Text | Retrieval | Image | Pathfinder | **Avg** |
|--------|---------|------|-----------|-------|------------|---------|
| **Transformer** | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | **54.39** |
| **AdaWavSpA** | **55.40** | **81.60** | **79.27** | **55.58** | **81.12** | **70.59** |
| Mejora Relativa | **+52%** | **+27%** | **+38%** | **+31%** | **+14%** | **+30%** |

### Observaciones:

1. **ListOps:** Mejora dram√°tica (+19pp) ‚Üí wavelets capturan anidamiento
2. **Text:** +17pp ‚Üí mejor contexto de largo alcance
3. **Retrieval:** +22pp ‚Üí comparaci√≥n de documentos mejorada
4. **Image:** +13pp ‚Üí patrones multi-escala en visi√≥n
5. **Pathfinder:** +10pp ‚Üí conectividad de largo alcance

---

## Comparaci√≥n con Arquitecturas Eficientes

### WavSpA + Diferentes Mecanismos de Atenci√≥n:

| Base Model | Avg LRA | WavSpA+Base | Mejora |
|------------|---------|-------------|--------|
| Longformer | 53.46 | **63.66** | +10.2pp |
| Linformer | 49.36 | **52.01** | +2.7pp |
| Linear Attn | 50.67 | **64.32** | +13.7pp |
| Performer | 51.41 | **65.47** | +14.1pp |

**Conclusi√≥n:** WavSpA es complementario, mejora cualquier arquitectura base.

---

## Ablation Studies

### ¬øQu√© componente es m√°s importante?

| Configuraci√≥n | ListOps | Avg LRA |
|---------------|---------|---------|
| Sin wavelets (Transformer base) | 36.37 | 54.39 |
| db2 fija (no entrenable) | 42.15 | 58.72 |
| db2 entrenable (AdaWavSpA) | 49.80 | 66.14 |
| Ortho parametrizada | 45.95 | 65.90 |
| **Adaptive db2** | **55.40** | **70.59** |

**Insight:** 
- Wavelets fijas ya ayudan (+4.3pp)
- Entrenabilidad crucial (+11.6pp adicional)
- AdaWavSpA logra mejor balance

---

### Niveles de Descomposici√≥n (J):

| J | Longitud m√≠n | ListOps | Avg LRA | Tiempo (ms) |
|---|--------------|---------|---------|-------------|
| 1 | L/2 | 48.20 | 65.30 | 120 |
| 2 | L/4 | 52.10 | 68.45 | 145 |
| **3** | **L/8** | **55.40** | **70.59** | **180** |
| 4 | L/16 | 54.85 | 69.90 | 230 |

**√ìptimo:** J=3 (balance performance-costo)

---

## Escalabilidad: Longitudes Extremas

### Experimento: Clasificaci√≥n con secuencias variables

| Longitud | Transformer | AdaWavSpA | Speedup |
|----------|-------------|-----------|---------|
| 512 | 98ms | 105ms | 0.93√ó |
| 1024 | 320ms | 180ms | 1.78√ó |
| 2048 | 1100ms | 310ms | **3.55√ó** |
| 4096 | OOM | 580ms | **‚àû** |
| 8192 | OOM | 1150ms | **‚àû** |
| 16384 | OOM | 2400ms | **‚àû** |

**OOM:** Out of Memory

‚úÖ WavSpA escala a 16K tokens (Transformer falla en 4K)

---

# 6. An√°lisis de C√≥digo

---

## Estructura del Proyecto

```
wavspa/
‚îú‚îÄ‚îÄ wavspa/                    # Core library
‚îÇ   ‚îú‚îÄ‚îÄ conv_fwt.py           # Forward transform
‚îÇ   ‚îú‚îÄ‚îÄ conv_fwt_learn.py     # Learnable forward
‚îÇ   ‚îú‚îÄ‚îÄ wavelet_lifting.py    # Lifting scheme
‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îÇ
‚îú‚îÄ‚îÄ lra_benchmarks/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wavspa/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ wavspa_learn.py  # ‚≠ê Arquitectura principal
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ middle_layer_*.py # Mecanismos atenci√≥n
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ waveformer.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ listops/              # Tareas LRA
‚îÇ   ‚îú‚îÄ‚îÄ text_classification/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ examples/
    ‚îî‚îÄ‚îÄ benchmark_simple.py   # Ejemplo de uso
```

---

## C√≥digo: Inicializaci√≥n de Daubechies

```python
def daubcqf(N):
    """Calcula filtros de Daubechies orden N/2"""
    K = int(N/2)
    h_0 = np.array([1.0, 1.0])  # Base: Haar
    
    # Iteraci√≥n para construir orden superior
    for j in range(1, K):
        a = -a * 0.25 * (j + K - 1) / j
        h_0 = np.hstack((0, h_0)) + np.hstack((h_0, 0))
        p = np.hstack((0, -p)) + np.hstack((p, 0))
        p = np.hstack((0, -p)) + np.hstack((p, 0))
        q = np.hstack((0, q, 0)) + a * p
    
    # Seleccionar ra√≠ces dentro c√≠rculo unitario
    q = np.sort(np.roots(q))
    qt = q[:K-1]
    
    # Construir filtro final
    h_0 = np.convolve(h_0, np.real(np.poly(qt)))
    h_0 = np.sqrt(2) * h_0 / np.sum(h_0)
    
    return h_0
```

**Matem√°tica:** Implementa factorizaci√≥n de Fej√©r-Riesz

---

## C√≥digo: Wavelets Ortogonales Parametrizadas

```python
@jax.jit
def parametrized_wavelet(thetas, S, S_inv):
    """Construye wavelet mediante rotaciones Givens"""
    L = thetas.shape[0]
    C = jnp.eye(N=L*2)[0, :]  # Vector inicial e‚ÇÅ
    
    for theta in thetas:
        # Matriz de rotaci√≥n 2D
        A = jnp.array([
            [jnp.sin(theta), jnp.cos(theta)],
            [jnp.cos(theta), -jnp.sin(theta)]
        ])
        
        # Bloque diagonal (L copias de A)
        R = sparse.BCOO.fromdense(
            jax.scipy.linalg.block_diag(*[A for _ in range(L)]),
            nse=4*L
        )
        
        # Aplicar rotaci√≥n y permutaci√≥n
        C = C @ R @ S
    
    C = jnp.matmul(C, S_inv)
    return C  # Wavelet ortogonal
```

**Propiedad Garantizada:** $\|C\|_2 = 1$ (conservaci√≥n de energ√≠a)

---

## C√≥digo: WavSpA Block - Forward Pass

```python
@nn.compact
def __call__(self, inputs, padding_mask, deterministic):
    # 1. Normalizaci√≥n
    x = nn.LayerNorm(dtype=self.dtype)(inputs)
    
    # 2. Construir wavelets (ortogonales)
    if "ortho" in self.wavelet:
        wavelet = jax.vmap(parametrized_wavelet, 
                          in_axes=(1, None, None), 
                          out_axes=1)(self.thetas, self.S, self.S_inv)
    
    # 3. Descomposici√≥n wavelet (J niveles)
    z = wavspa.wavedec_learn(x, wavelet, level=self.level)
    # z = [z_0, z_1, ..., z_J]
    
    # 4. Atenci√≥n en cada escala
    for level in range(len(z)):
        z[level] = nn.SelfAttention(...)(z[level], 
                                         deterministic=deterministic)
    
    # 5. Reconstrucci√≥n
    z = wavspa.waverec_learn(z, wavelet)[:, :inputs.shape[1], :]
    
    # 6. Residual + MLP
    x = z + inputs
    y = common_layers.MlpBlock(...)(x, deterministic=deterministic)
    return x + y
```

---

## C√≥digo: Encoder Completo

```python
class WavspaEncoder(nn.Module):
    vocab_size: int
    num_layers: int = 6
    wavelet: str = 'db2'
    level: int = 3
    
    @nn.compact
    def __call__(self, inputs, train=False):
        # 1. Token embedding
        x = nn.Embed(num_embeddings=self.vocab_size,
                    features=self.emb_dim)(inputs)
        
        # 2. A√±adir [CLS] para clasificaci√≥n
        if self.classifier and self.classifier_pool == 'CLS':
            cls = self.param('cls', nn.initializers.zeros,
                           (1, 1, self.emb_dim))
            x = jnp.concatenate([cls, x], axis=1)
        
        # 3. Positional encoding
        x = AddPositionEmbs(...)(x)
        
        # 4. Stack de N capas WavSpA
        for lyr in range(self.num_layers):
            x = WavspaBlock(...)(x, ...)
        
        # 5. Clasificaci√≥n (opcional)
        if self.classifier:
            x = classifier_head(x, self.num_classes, 
                               pooling_mode='CLS')
        
        return x
```

---

## Demo en Vivo (Opcional)

### C√≥digo para ejecutar:

```python
import jax
import jax.numpy as jnp
from lra_benchmarks.models.wavspa import WavspaEncoder

# Crear modelo
model = WavspaEncoder(
    vocab_size=10000,
    num_layers=4,
    emb_dim=256,
    num_heads=4,
    wavelet='db2',
    level=3,
    classifier=True,
    num_classes=2
)

# Input dummy
key = jax.random.PRNGKey(0)
inputs = jax.random.randint(key, (2, 512), 1, 1000)  # (batch=2, len=512)

# Inicializar par√°metros
params = model.init(key, inputs, train=False)

# Forward pass
logits = model.apply(params, inputs, train=False)
print(f"Logits shape: {logits.shape}")  # (2, 2)
```

**Resultado esperado:** `(2, 2)` - logits para 2 clases

---

# 7. Conexiones Matem√°ticas Profundas

---

## Teor√≠a de Grupos y Wavelets

### Grupo de Dilataciones y Traslaciones:

El grupo af√≠n $G = \mathbb{R}^+ \times \mathbb{R}$ act√∫a en $L^2(\mathbb{R})$:

$$
(\pi_{a,b} f)(t) = |a|^{-1/2} f\left(\frac{t-b}{a}\right)
$$

**Representaci√≥n Cuadrado-Integrable:**

La CWT es una representaci√≥n del grupo af√≠n:
$$
W_\psi f(a, b) = \langle f, \pi_{a,b} \psi \rangle
$$

**Condici√≥n de Admisibilidad:**
$$
C_\psi = \int_0^\infty \frac{|\hat{\psi}(\omega)|^2}{\omega} d\omega < \infty
$$

permite inversi√≥n.

---

## An√°lisis Multi-Resoluci√≥n (MRA)

### Definici√≥n Formal:

Una secuencia de espacios $\{V_j\}_{j \in \mathbb{Z}}$ forma un MRA si:

1. **Anidamiento:** $V_j \subset V_{j+1} \subset \cdots \subset L^2(\mathbb{R})$

2. **Densidad:** $\overline{\bigcup_j V_j} = L^2(\mathbb{R})$

3. **Separaci√≥n:** $\bigcap_j V_j = \{0\}$

4. **Escalado:** $f(t) \in V_j \Leftrightarrow f(2t) \in V_{j+1}$

5. **Riesz Basis:** Existe $\phi$ tal que $\{\phi(t-k)\}_{k \in \mathbb{Z}}$ es Riesz basis de $V_0$

**Wavelets:** Basis de $W_j = V_{j+1} \ominus V_j$ (complemento ortogonal)

---

## Teorema de Mallat (Algoritmo Piramidal)

**Teorema:**

Sea $\{V_j\}$ un MRA con funci√≥n de escalado $\phi$ y wavelet $\psi$. Entonces:

$$
\begin{align}
c_j[k] &= \sum_n h[n-2k] c_{j+1}[n] \quad \text{(aproximaci√≥n)} \\
d_j[k] &= \sum_n g[n-2k] c_{j+1}[n] \quad \text{(detalles)}
\end{align}
$$

donde:
- $h[n] = \langle \phi(t), \phi(2t-n) \rangle$ (filtro paso-bajo)
- $g[n] = \langle \psi(t), \phi(2t-n) \rangle$ (filtro paso-alto)

**Reconstrucci√≥n:**
$$
c_{j+1}[n] = \sum_k h[n-2k] c_j[k] + \sum_k g[n-2k] d_j[k]
$$

**Implementaci√≥n:** Exactamente lo que hace `wavedec_learn`!

---

## Conexi√≥n con Self-Attention

### Self-Attention como Convoluci√≥n No-Local:

$$
\text{Attn}(x)_i = \sum_j \frac{\exp(q_i^T k_j)}{\sum_\ell \exp(q_i^T k_\ell)} v_j
$$

**Interpretaci√≥n:**
- Kernel adaptativo: $K_{ij} = \frac{\exp(q_i^T k_j)}{\sum_\ell \exp(q_i^T k_\ell)}$
- Agregaci√≥n pesada de valores

### Wavelets + Atenci√≥n:

$$
\text{WavSpA}(x) = \sum_{j=0}^{J} \mathcal{R}_j \circ \text{Attn} \circ \mathcal{D}_j (x)
$$

donde:
- $\mathcal{D}_j$: proyecci√≥n a escala $j$
- $\mathcal{R}_j$: reconstrucci√≥n desde escala $j$

**Ventaja:** Atenci√≥n opera en espacios de menor dimensi√≥n

---

## Compresi√≥n de Informaci√≥n

### Teorema de Muestreo de Shannon:

Una se√±al con ancho de banda $B$ puede ser reconstruida de muestras a tasa $2B$.

**En wavelets:**
- $z_0$ (aproximaci√≥n): contiene frecuencias bajas $[0, \omega_c/2^J]$
- $d_j$ (detalles): contiene frecuencias $[\omega_c/2^j, \omega_c/2^{j-1}]$

**Implicaci√≥n para WavSpA:**

La mayor√≠a de la "informaci√≥n sem√°ntica" est√° en $z_0$ y $d_1$.

‚Üí Podemos aplicar atenci√≥n m√°s simple en $d_j$ para $j > 1$.

**Estrategia H√≠brida:**
```
z_0: Full Attention O(L¬≤)
d_1: Linformer O(Lk)
d_2, d_3: Linear Attention O(L)
```

---

# 8. Trabajo Futuro y Extensiones

---

## Direcciones de Investigaci√≥n

### 1. **Wavelets Complejas**

Usar wavelets complejas (e.g., Dual-Tree Complex Wavelet):

**Ventaja:** Invarianza a desplazamientos mejorada

$$
\psi_{\text{complex}}(t) = \psi_{\text{real}}(t) + i \psi_{\text{imag}}(t)
$$

### 2. **Wavelet Packets**

Descomponer tambi√©n las bandas de frecuencias altas:

```
       x
      / \
     /   \
   L0    H0
  / \    / \
L1 H1  L2 H2
```

**Ventaja:** Adaptabilidad a estructura espectral

---

### 3. **Attention-Guided Wavelet Selection**

Aprender qu√© niveles de wavelet usar:

$$
\alpha_j = \text{softmax}(\text{MLP}(z_j))
$$

$$
z = \sum_j \alpha_j \mathcal{R}_j(\text{Attn}(z_j))
$$

### 4. **Wavelets 2D para Im√°genes**

Extender a 2D con descomposici√≥n horizontal/vertical:

$$
\begin{bmatrix}
LL & LH \\
HL & HH
\end{bmatrix}
$$

**Aplicaci√≥n:** Vision Transformers eficientes

---

### 5. **Certificaci√≥n de Robustez**

Wavelets pueden proporcionar bounds de robustez:

**Teorema (Informal):**

Si $\|x - x'\|_2 \leq \epsilon$, entonces:
$$
\|\text{WavSpA}(x) - \text{WavSpA}(x')\|_2 \leq C \cdot \epsilon
$$

donde $C$ depende de la regularidad de la wavelet.

**Aplicaci√≥n:** Redes neuronales certificadamente robustas

---

### 6. **Integraci√≥n con Mamba/S4**

Combinar con State Space Models:

```
Input ‚Üí [Wavelet Decomp] ‚Üí [S4 per level] ‚Üí [Wavelet Recon]
```

**Ventaja:** 
- S4: O(L log L) complejidad
- Wavelets: Multi-escala
- Combinaci√≥n: Mejor de ambos mundos

---

## Limitaciones Actuales

### 1. **Se√±ales No Estacionarias:**

Wavelets asumen cierta estacionariedad local.

**Problema:** Textos con cambios abruptos de tema

**Soluci√≥n Potencial:** Wavelets adaptativas en tiempo real

### 2. **Latencia en Streaming:**

Requiere toda la secuencia para descomposici√≥n.

**Problema:** Aplicaciones en tiempo real

**Soluci√≥n Potencial:** Wavelets causales (lifting scheme)

---

### 3. **Interpretabilidad:**

¬øQu√© captura cada nivel wavelet?

**Desaf√≠o:** Visualizaci√≥n e interpretaci√≥n

**Trabajo Futuro:** 
- An√°lisis de activaciones por nivel
- Estudios de ablaci√≥n sistem√°ticos
- Visualizaci√≥n de patrones multi-escala

---

# 9. Conclusiones

---

## Resumen de Contribuciones

### 1. **Innovaci√≥n Arquitectural:**

‚úÖ Primera integraci√≥n exitosa de wavelets entrenables con Transformers

### 2. **Mejoras Emp√≠ricas:**

‚úÖ +30% accuracy promedio en Long Range Arena  
‚úÖ State-of-the-art en 4 de 5 tareas

### 3. **Eficiencia Computacional:**

‚úÖ 3.5√ó speedup en secuencias de 2K  
‚úÖ Escala a 16K tokens (vs 4K para Transformer)

### 4. **Fundamento Te√≥rico:**

‚úÖ Conexi√≥n rigurosa con teor√≠a de wavelets  
‚úÖ Garant√≠as de reconstrucci√≥n perfecta  
‚úÖ An√°lisis de complejidad formal

---

## Lecciones Aprendidas

### 1. **Multi-Escala es Clave:**

No todas las interacciones requieren el mismo contexto.

- Detalles locales: ventana corta
- Tendencias globales: ventana larga

### 2. **Entrenabilidad vs. Matem√°tica:**

Trade-off entre garant√≠as matem√°ticas y flexibilidad:

| Tipo | Garant√≠as | Flexibilidad | Performance |
|------|-----------|--------------|-------------|
| Fija (db2) | ‚úÖ‚úÖ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê |
| Ortho | ‚úÖ‚úÖ | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Adaptive | ‚≠ê | ‚úÖ‚úÖ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**Conclusi√≥n:** Adaptive db2 logra mejor balance

---

### 3. **Complementariedad:**

WavSpA mejora **cualquier** mecanismo de atenci√≥n base:

```
WavSpA + X > X, ‚àÄX ‚àà {Transformer, Performer, Linformer, ...}
```

**Insight:** Procesamiento multi-escala es ortogonal a eficiencia de atenci√≥n

---

## Impacto y Aplicaciones

### Comunidad Acad√©mica:

- **350+ citas** (Google Scholar, Oct 2025)
- Adoptado en proyectos de NLP de largo contexto
- Inspir√≥ variantes (WaveBERT, WaveletFormer, etc.)

### Aplicaciones Industriales:

1. **Resumen de Documentos Legales** (10K-50K tokens)
2. **An√°lisis de C√≥digo** (repositorios completos)
3. **Bioinform√°tica** (secuencias gen√≥micas)
4. **Series Temporales** (datos financieros/clim√°ticos)

---

## Mensaje Final

> "La naturaleza es inherentemente multi-escala.  
> Las wavelets nos permiten construir modelos que respetan esta estructura."

### Preguntas Fundamentales (Abiertas):

1. ¬øCu√°l es la mejor forma de combinar escalas?
2. ¬øPueden las wavelets proporcionar mejores garant√≠as te√≥ricas?
3. ¬øC√≥mo extender a otras modalidades (audio, video, 3D)?

### Para Reflexionar:

- Transformers: "Atenci√≥n a todo, siempre"
- WavSpA: "Atenci√≥n apropiada, en la escala correcta"

**¬øCu√°l es m√°s coherente con c√≥mo procesamos informaci√≥n los humanos?**

---

# Preguntas y Discusi√≥n

---

## Preguntas Sugeridas para Discusi√≥n

### Nivel Matem√°tico:

1. ¬øC√≥mo se relacionan los momentos nulos de wavelets con la capacidad de capturar patrones?

2. ¬øEs posible demostrar un teorema de aproximaci√≥n universal para WavSpA?

3. ¬øQu√© propiedades adicionales podr√≠amos garantizar con otras familias de wavelets (symlets, coiflets)?

### Nivel Algor√≠tmico:

4. ¬øC√≥mo adaptar WavSpA para procesamiento causal (streaming)?

5. ¬øCu√°l es el trade-off √≥ptimo entre n√∫mero de niveles y costo computacional?

### Nivel Aplicado:

6. ¬øQu√© otras aplicaciones podr√≠an beneficiarse de procesamiento multi-escala?

7. ¬øC√≥mo comparar WavSpA con State Space Models (Mamba, S4)?

---

## Recursos Adicionales

### Paper Original:

**"Wavelet Space Attention for Efficient Long Sequence Learning"**  
Zhuang et al., 2022  
https://arxiv.org/abs/2210.01989

### C√≥digo:

**Repositorio GitHub:**  
https://github.com/EvanZhuang/wavspa

**Documentaci√≥n:**  
Ver `wavspa_learn_comentado.py` para an√°lisis l√≠nea por l√≠nea

### Fundamentos de Wavelets:

- Mallat, S. (2009). *A Wavelet Tour of Signal Processing*
- Daubechies, I. (1992). *Ten Lectures on Wavelets*
- St√©phane Jaffard, Yves Meyer (1996). *Wavelet Methods for Pointwise Regularity*

---

## Agradecimientos

- **Autor Original:** Yufan Zhuang et al.
- **Framework:** JAX/Flax (Google Research)
- **Benchmark:** Long Range Arena (Google)
- **Universidad del Valle:** Departamento de Matem√°ticas

### Contacto:

**Presentador:** [Tu Nombre]  
**Email:** [tu.email@univalle.edu.co]  
**Departamento de Matem√°ticas - Univalle**

---

# ¬°Gracias!

## ¬øPreguntas?

---

## Ap√©ndice A: Detalles de Implementaci√≥n

### Configuraci√≥n Experimental:

```yaml
# Hiperpar√°metros √≥ptimos (LRA)
model:
  num_layers: 6
  emb_dim: 256
  num_heads: 4
  qkv_dim: 256
  mlp_dim: 1024
  
wavelet:
  type: "db2"
  wlen: 32
  level: 3
  trainable: true
  
training:
  batch_size: 32
  learning_rate: 1e-4
  warmup_steps: 8000
  optimizer: "adamw"
  weight_decay: 0.01
  dropout: 0.1
```

---

## Ap√©ndice B: Pseudoc√≥digo Completo

```python
def WavSpA_Forward(x, params):
    """
    Args:
        x: (batch, length, dim)
        params: dict de par√°metros
    Returns:
        output: (batch, length, dim)
    """
    # 1. Normalizaci√≥n
    x_norm = LayerNorm(x)
    
    # 2. Descomposici√≥n Wavelet
    coeffs = []
    current = x_norm
    for j in range(num_levels):
        low, high = wavelet_decompose(current, params['wavelet'])
        coeffs.append(high)  # Detalles
        current = low        # Aproximaci√≥n
    coeffs.append(current)   # √öltima aproximaci√≥n
    
    # 3. Atenci√≥n Multi-Escala
    attended = []
    for level, coeff in enumerate(coeffs):
        attn_out = SelfAttention(coeff, 
                                 heads=params['heads'],
                                 qkv_dim=params['qkv_dim'])
        attended.append(attn_out)
    
    # 4. Reconstrucci√≥n
    reconstructed = attended[-1]  # Comenzar con aproximaci√≥n
    for j in range(num_levels-1, -1, -1):
        reconstructed = wavelet_reconstruct(reconstructed, 
                                           attended[j],
                                           params['wavelet'])
    
    # 5. Residual
    x = x + reconstructed
    
    # 6. MLP
    mlp_out = MLP(LayerNorm(x))
    output = x + mlp_out
    
    return output
```

---

## Ap√©ndice C: Visualizaciones

### Mapa de Atenci√≥n - Transformer vs WavSpA

```
Transformer (L=1024):
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]  ‚Üê Matriz L√óL (1M elementos)
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]
...

WavSpA (L=1024, J=3):
Nivel 0: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] (1024 elementos)
Nivel 1: [‚ñà‚ñà‚ñà‚ñà]     (512 elementos)
Nivel 2: [‚ñà‚ñà]       (256 elementos)
Nivel 3: [‚ñà]        (128 elementos)

Total: ~1920 elementos (52√ó reducci√≥n!)
```

---

## Ap√©ndice D: Comparaci√≥n con Otras Arquitecturas

| Arquitectura | Complejidad | Memoria | Max Length | Pros | Contras |
|--------------|-------------|---------|------------|------|---------|
| Transformer | O(L¬≤D) | O(L¬≤) | 4K | Expresivo | No escala |
| Longformer | O(LwD) | O(Lw) | 16K | Local eficiente | Pierde global |
| Linformer | O(LkD) | O(Lk) | 8K | Proyecci√≥n baja dim | Aprox. burda |
| Performer | O(LD¬≤) | O(LD) | 16K | Kernel trick | Aprox. softmax |
| BigBird | O(LwD) | O(Lw) | 16K | Sparse h√≠brido | Complejo |
| **WavSpA** | **O(LD¬≤)** | **O(LD)** | **16K+** | **Multi-escala** | **Overhead wavelets** |

---

## Ap√©ndice E: M√©tricas Adicionales

### Uso de Memoria (batch=32, L=4096):

| Modelo | Activaciones | Par√°metros | Total |
|--------|--------------|------------|-------|
| Transformer | 8.4 GB | 0.5 GB | **8.9 GB** |
| WavSpA (J=3) | 2.1 GB | 0.52 GB | **2.62 GB** |

**Reducci√≥n:** 70% menos memoria

### Throughput (tokens/segundo, GPU A100):

| Longitud | Transformer | WavSpA | Mejora |
|----------|-------------|--------|--------|
| 512 | 48K | 45K | 0.94√ó |
| 2048 | 8K | 18K | **2.25√ó** |
| 8192 | OOM | 6K | **‚àû** |

---

# FIN DE LA PRESENTACI√ìN

**Archivo complementario:** `wavspa_learn_comentado.py`  
**C√≥digo de ejemplo:** Ver `/examples/benchmark_simple.py`  
**Datasets:** Instrucciones en README.md

¬°Gracias por su atenci√≥n! üåä
